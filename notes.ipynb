{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1: Getting data from APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build an application that predicts stock volatility\n",
    "- get stock data from API\n",
    "- create python classes to extract data from API and load it into database\n",
    "- build a GARCh time series model to predict volatility\n",
    "- build an application with its own API for getting data, training models and making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goals:\n",
    "- export stocj data from the Alpha Vantage API using a URL\n",
    "- Extract stock data from the AlphaVantage API using an HTTP request\n",
    "- Write function for transforming stock data\n",
    "- incorporate python exceptions into our function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing APIs through URL\n",
    "- Identify components of a URL\n",
    "- Add API key to a config module\n",
    "- Incorporate AlphaVantage parameters into URL\n",
    "Accessing APIs throgh an HTTP request\n",
    "- Define an HTTP request\n",
    "Defensive Programming for APIs\n",
    "- Create get_daily function\n",
    "- Raise Exceptions for bad requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's best to store time series data in descending order for storage, with the most recent date at the top. It is advantageous when retrieving data from an application database. Ascending order for model training for time series data since the earliest date is meant to come first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, errors that could disrupt the code are\n",
    "- putting in a bad API path\n",
    "- putting in an invalid ticker symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it's best to  program defensively or implement error-handling in code. to accomplish this, try an anticipate errors that could occur. also, try to inform the user of the error in case the exception is triggered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lessons learnt:\n",
    "- learned how to extract data from the Alpha Vantage A|PI using a URL\n",
    "    - learned about important components of the URL like hostname (base URL), path (particular action you want to do when you hit that URL), parameters (arguments passed into the URL)\n",
    "- learned about API keys\n",
    "    - DO NOT put them in your code.\n",
    "    - use a .env file to store the API key as an environment variable\n",
    "- Extracted data from the AlphaVantage API programmatically using a HTTP request.\n",
    "    - used request library to make request and get a response.\n",
    "- Figured out how to take the JSON and turn it into a datframe\n",
    "- Cleaned eveything up by putting it all in a functionthat takes input from a user \n",
    "- Raise exceptions when we anticipate possible user errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2: Test-driven development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create class for interfacing with AlphaVantage API and getting data\n",
    "- create a SQL repository class for storing and retreiving stock data.\n",
    "- use test-driven development strategy. \n",
    "    - We decide what functionality to include in classes before we start coding \n",
    "    - write assert statement to test the work before it is done.\n",
    "- calculate and compare stock for 2 companies in india by comparing daily returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data module\n",
    "\n",
    "- AlphaVantage API\n",
    "    - class definition\n",
    "- SQLRepository API\n",
    "    - using assert statements before coding \\insert_table method\n",
    "    - read_table method\n",
    "- stock returns\n",
    "    - plotting and comparing closing stock price\n",
    "    - calculating returns\n",
    "    - plottinng abd comparing stock returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using auto_reload extension for dynamic testing in Notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Data Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlphVantage API Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- adding a double underscore or '__' as the first 2 characters of a class attribute make it a secret attribute. you can access the attributre but it doesn't appear when you do 'dir'\n",
    "- turn get_daily function into a class method\n",
    "- need to create tests to make sure method is working properly\n",
    "- test driven development (TDD)\n",
    "    - uou create a series of assert sratements that tests the outputs of functions or methods that you need to write bnefore you write it.\n",
    "    - when yoiu are done codeing, use those assert statements to confirm that the code functions as expected\n",
    "    - in the short term it makes things easier and allows one to focuss on end goal as you build the software\n",
    "    in the longer term, helps with maintenance becasue everytime changes are made to the code the same tests are re-used to make sure everyting is working fine\n",
    "    - check for the following\n",
    "        - if the get_daily method returns a dataframe\n",
    "        - the number of columns of the dataframe\n",
    "        - whether the dataframe has a datetimeindex\\\n",
    "        - whether the index name is date\n",
    "        - if all columns have the right column names\n",
    "        - if all columns have the right data type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Repository Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create a connection to sqlite to create a database\n",
    "- set check_same_thread to help manage fast APIs (read up on this later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLOSING PRICES ARE SO DIFFRENT IN ORDER OF magnitude between the 2 companies that it looks like suzlon is a joke. however, there are other factors to consider.\n",
    "\n",
    "a way to get around it is to examine stock return as opposed to the closing price.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3: Predicting Volatility with GARCH model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goals:\n",
    "- create wrangle_data function for calculating returns\n",
    "- explore volatility for Ambuja and suzlon stocks\n",
    "- build GARCH model to predict Ambuja volatility\n",
    "- create a clean_prediction function for formating model predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "workflow:\n",
    "- prepare data\n",
    "    - import: sql repository, wrangle_data\n",
    "    - explore\n",
    "        - time series and non-time-series plots\n",
    "        - squared returns (need this to create ACF and PACF plots)\n",
    "        - split\n",
    "    - mean, variance, standard deviation\n",
    "    - rolling window for volatility\n",
    "- build model\n",
    "    - iterate: GARCH model, standardized residuals\n",
    "    - evaluate: walk forward validation\n",
    "    - conditional volatility\n",
    "- communicate results\n",
    "    - convert timestamp to ISO 8601\n",
    "    - clean prediction model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conditional vs un-conditional volatility\n",
    "- look at volatility accorss time and divorced from time\n",
    "- time series plot shows volatility across time. past to present. \n",
    "    - this can also be called conditional volatility\n",
    "    - that is volatility that changes aacross time\n",
    "- unconditional volatility\n",
    "    - looking at the measured value without considering time\n",
    "    - volatility is the same as standard deviation they are just in different fields\n",
    "    - there are 252 trading days out of 365 in a year.\n",
    "    - annual volatility is bigger than daily volatility becasue day to day stocks can change or have an extreme changes. but adding uip all those changes in the timespan of a year would produce a large amount of volatility.\n",
    "    - calculated daily and annual volatilities.\n",
    "- conditional volatility\n",
    "    - a great way to examine how volatility can change across time is a rolling window.\n",
    "    - the return is the percent change in a stock from day to day\n",
    "    - volatility during covid was more\n",
    "    - as spread goes wider volatility goes u. as spread come closer volatility goes down.\n",
    "- autocorrelation?\n",
    "    - need to consider if there is a relationship beteen  daily return on one day and the day before that or that day and the day before that.\n",
    "    - interested in the size of the jump in values of either positive or negative daily returns.\n",
    "    - a good way to handle this would be to square the values.\n",
    "    - the squared returns represent volatility as a magnitude or height above 0 now that the daily returns were squared. the latter represented a spread centered around 0.\n",
    "    - Periods of high and low volatitility that tend to cluster together.\n",
    "    - this pattern shows why a GARCH Model is a good fit.\n",
    "- ACF\n",
    "     - find out if there is a relationship or a correlation between the volatility on one day and the volatility on days prior. or prior timestamps or lags.\n",
    "- PACF\n",
    "    - really tells us how many lags we need\n",
    "    - according to the pacf, it looks like a lag of 2 or 3 would be a good starting point.\n",
    "- split\n",
    "    - in time series data we don't randomly assign data to train or test because time only moves in one direction.\n",
    "    - take first 80% of observations as training set. remaining 20% as test set.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- build model\n",
    "    - how GARCH works\n",
    "        - common in finance for predicting volatility\n",
    "        - stands for generalised auto regressive conditional heteroskedasticity\n",
    "        - main points\n",
    "            - it does not predict volatility, it predicts variance.\n",
    "            - variance is the square of standard deviation\n",
    "            - the auto regressive part means making a prediction about the future based on information from the past.\n",
    "            - the conditional heteroskedasticity means that the models prediction can change over time. that is it'll be different at each timestamp which is good because volatility of ambuja cement also changes over time.\n",
    "        - Garch equation: interest is predicting the volatility of a stock on a given day or standard deviation of a stock's returns.\n",
    "            - sigma^2 subscript t: variance/volatility for a specific time.\n",
    "            - omega (small w): this is the longrun average variance of the stock returns. steady component of a stocks variance. doesn't change from day to day.it's always there contributing to the overall variance of the stock. the model estimates this when it is fitted to the training data.\n",
    "            - alpha: all about task information. where you put returns from previous timestamps. the part of the model that takes that into account. it is a coefficient that the model needs to estimate based on the training.\n",
    "            - beta: all about past predictions. where you put the model's predictions from previous time stamps. it is also a coefficient made from trainign the model\n",
    "            - by estimating the values for omega, alpha and beta the model is trying to figure out what balance between these 3 leads to the best predictions overall.\n",
    "            - these coefficients are going to be very small\n",
    "            - this equation is for a GARCH (1,1) model which means 1 lag for alpha term, 1 lag for beta term.\n",
    "            - based on the results from the pacf plot we will use a garch (3,3)\n",
    "    - the model parameters \n",
    "        - for the arch model are the raining set, p and q (the lags we want for p and q)\n",
    "        - p is the number of alpha terms we have in the equation\n",
    "        - q is the number of beta terms in the equation\n",
    "        - rescale is set to false because it deals with how the model fits\n",
    "    - model summary\n",
    "        - aic and bic scores are measurements of how well the model fits the data and the balance of the complexity of the model. they should be as low as possible.\n",
    "        - the p scores tells us the level of statistical significance of the coefficients\n",
    "        - in any model that uses coefficients will have a statistical significance for each coeffiecient. this applies to regression, ARMA anything. the goal is to have a model that has every coefficient being statistically significant.\n",
    "        - the p scores should all be lower than 0.05 to be statistically significant.\n",
    "        - the lags were changed from 3 to 2 to 1 mainly because not all p scores were statistically significant. also with a lag of 1 the aic and bic of the model were reduced to the lowest. so at lag 1 the model performed its best.\n",
    "        - this shows that the best bet is to have a garch model that is (1,1). 1 lag for p and 1 lag for q. which is the most common in finance.\n",
    "    - model prediction plots\n",
    "        - plot predicts to see if it mimics the volatility that we see in the daily returs\n",
    "        - plot training data together with model predictions\n",
    "        - model is more or less following the data.\n",
    "        - volatility can go both ways. it can be a big jump or a big drop.\n",
    "        - to see if the mnodel actually follows the volatilty of the data, we can raise it but multiplying the values by 2. \n",
    "        - whether positive or negative the model is mimicking the volatility of the returns.\n",
    "    - standardized residuals\n",
    "        - with a garch model you don't examine normal residuals. you look at standardized residuals\n",
    "        - a look at residuals across time\n",
    "        - there should be no trends, it sould just be consistent static whcih ,eans it has a consistent mean and spread over time.\n",
    "    - histograms of standardized residuals\n",
    "        - look at residuals divorced from time to see if they have a normal distribution.\n",
    "        - histogram is centered on zero with a normal distribution.\n",
    "        - need to check for any auto-correlation.\n",
    "    - ACF of standardized residuals\n",
    "        - check if there are any remaining autocorellations in the residuals.\n",
    "        - since resifuals are positive are negative, we need to square them to make all of them positive\n",
    "        - all residuals are in the blue bar whichy means nothing is significant.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- time to make forcasts for dates the model did not see during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- one day forecast\n",
    "    - this is to get the predicted volatility of the next day after the last day in the training set.\n",
    "    - horizon argument represents how many days you want to predict into the future.\n",
    "    - reindex argument is set to false to return a smaller object\n",
    "    - result is a dataframe with 2 components\n",
    "        - a datetime index with the only date being the last date in the training set\n",
    "        - one feature, which is a prediction for the next day. specifically, it is thje prediction for the last day of the training data with a horizon of 1.\n",
    "        - need just that prediction from the model.\n",
    "        - the model produces the variance but the focus is on volatility which is otherwise know as standard deviation. Std is the square root of variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communicate Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- walk forward validation\n",
    "    - 20% for test set\n",
    "    - the for loop \n",
    "        - looks at the data in the training set and with each successive loop you add one observation from the test set to y_train\n",
    "        - with each loop we retrain the model with the y_train\n",
    "        - generate next prediction\n",
    "        - append prediction to predictions list\n",
    "    - gtet predictions into a series with a datetime index.\n",
    "    - predicted volatility resembles the returns. in times of low volatility the predicted values reduce and in times of high volatility the predicted values increase.\n",
    "    - jung box test is another method used to evaluate data like this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Format timestamps\n",
    "    - need to take forecasts and reformat tyhem to work with web application. \n",
    "    - make sure all dates are in the right format.\n",
    "    - get 5 day forecast, get first day by looking at index and adding a day\n",
    "    - create a date range that starts at start date and goes for how many perios there are oin our prediction dataframe\n",
    "    - convert dates to ISO format\n",
    "- Clean_prediction Function\n",
    "    - extract values from prediction and then flatten the resulting numpy array.\n",
    "    - find the squareroot of the values to calculate the standard deviation or volatility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- created a wrangle data function to get stock data from database and calculate returns\n",
    "- explored the returns for the 2 companies and leaned what volatility is, which is the standard deviation of the returns of a stock\n",
    "- learned to think about volatility in a conditional and unconditional sense\n",
    "- learned about garch model and its 3 parts: long range variance, previous returns and previous predictions of those returns\n",
    "- created a clean prediction function for formating model predictions into a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4: Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GOALS\n",
    "    - create a garchmodel class with methods for wrangling data, training, predicting, saving, and loading models.\n",
    "    - build web API using FastAPI and uvicorn\n",
    "    - build data classes for API using pydantic\n",
    "    - create API paths for training ,odel and serving predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model module\n",
    "    - create garch model class definition\n",
    "    - add wrangle data model\n",
    "    - add fit and predict methods\n",
    "    - add dump and load methods\n",
    "    - error handling: ttry, except bloacks\n",
    "    - model permanence: dump, load\n",
    "\n",
    "- main module\n",
    "    - create FsdtAPI application with \"hello world\" example\n",
    "    - add \"/fit\" path to application, with data classes\n",
    "    - add \"/predict\" path to application with data classes\n",
    "    - Application path / endpoint\n",
    "    - Data classes: inheritance, type hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GarchMOdel class\n",
    "    - a garch model for a new indian company\n",
    "    - check to see if new model has a data attribute\n",
    "    - dump method\n",
    "        - a way to save the model to a pickle file\n",
    "        - every time we save a model it wil save a file in the model's directory with the exact time the model was saved with the ticker symbol for that model.\n",
    "    - load method\n",
    "        - goal is to write a method that will load the last saved model\n",
    "        - use glod library to search for specific file paths\n",
    "        - use error handling to account to models that have not been save yet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- launch server\n",
    "    - run server locally with uvicorn library\n",
    "    - to lauch the app locally use:\n",
    "        - uvicorn main:app --reload --workers 1 --host localhost --port 8008\n",
    "        - uvicorn launches the app\n",
    "        - reload is for automatic reloads\n",
    "        - number of workers is for parallel processes but with reload set to 1\n",
    "        - send request to port 8008\n",
    "- Hello path\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
